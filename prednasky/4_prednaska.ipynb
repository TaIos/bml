{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Přednáška 4: Lineární regresní model, regularizace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsah přednášky\n",
    "- regrese, formulace, k čemu je to dobré\n",
    "- řešení lineární regrese pomocí nejmenších čtverců\n",
    "- regularizace: co, proč, jak?\n",
    "- formulace problému pomocí optimalizace\n",
    "    - Tichonovova regularizace\n",
    "    - LASSO\n",
    "    - elastic net\n",
    "- bayesovská formulace problému (dokončení příště)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrese\n",
    "Mějme následující veličiny popisující problém:\n",
    "\n",
    "- $y$: náhodná pozorovaná veličina (např. měření)\n",
    "- $X$: daná reálná proměnná (např. vycházející z modelu daného problému)\n",
    "- $\\beta$: neznámé parametry, regresní koeficienty (ty se snažíme odhadnout)\n",
    "\n",
    "Regresní model pak můžeme zapsat jako vztah $y$ vzhledem k $X$ a $\\beta$:\n",
    "\n",
    "$$\n",
    "y \\approx f(X,\\beta),\n",
    "$$\n",
    "\n",
    "kde funkce $f()$ je specifická funkce, typicky zvolená dle daného problému a aplikace. My se budeme věnovat lineárnímu regresnímu modelu, tedy\n",
    "\n",
    "$$\n",
    "y=X\\beta,\n",
    "$$\n",
    "\n",
    "kde $\\mathbf{y} \\in \\mathbf{R}^{p\\times 1}$ je reálný vektor pozorování, $X \\in \\mathbf{R}^{p\\times n}$ je daná reálná matice a $\\beta \\in \\mathbf{R}^{n\\times 1}$ je neznámý vektor, který chceme odhadovat. Poznamenejme, že matice $X$ typicky obsahuje fyzikální podstatu úlohy, např. nějakou projekci, model průchodu materiálem atd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivace\n",
    "### rekonstrukce obrazu v tomografii\n",
    "\n",
    "<img src=\"img_ot/l4_tomography_example.JPG\">\n",
    "\n",
    "### elektronová mikroskopie\n",
    "\n",
    "<img src=\"img_ot/l4_el_mikro_example.jpg\">\n",
    "\n",
    "### rozpoznání a řídká reprezentace signálu\n",
    "\n",
    "<img src=\"img_ot/l4_sparse_example.png\">\n",
    "\n",
    "### odhad úniku škodlivin do ovzduší\n",
    "\n",
    "<img src=\"img_ot/l4_MadJod_Flexpart.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineární regrese\n",
    "Předpokládejme, že hodnotu každého měření můžeme vysvětlit jako\n",
    "$$\n",
    "y_i = \\mathbf{x}_i^T\\beta + \\mathbf{e}_i,\n",
    "$$\n",
    "dohromady pro všechna měření tedy jako\n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots\\\\\n",
    "y_{p}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{cccc}\n",
    "x_{11} & x_{12} &  & x_{1n}\\\\\n",
    "x_{21} & x_{22} &  & x_{2n}\\\\\n",
    " &  & \\ddots & \\vdots\\\\\n",
    "x_{p1} & x_{p2} & \\ldots & x_{pn}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}\n",
    "\\end{array}\\right)+\\mathbf{e}.\n",
    "$$\n",
    "Kompaktní zápis:\n",
    "$$\n",
    "\\mathbf{y} = X\\beta + \\mathbf{e}\n",
    "$$\n",
    "Podíváme se nejprve na řešení \"klasickými\" metodami.\n",
    "\n",
    "### Metoda nejmenších čtverců\n",
    "Hledáme co nejmenší součet čtverců odchylek od jednotlivých naměřených bodů, což je výraz\n",
    "$$\n",
    "\\sum_j e_j^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - X\\beta)^T(\\mathbf{y} - X\\beta).\n",
    "$$\n",
    "Extrém můžeme nalézt např. derivací podle $\\beta$ položenou rovno nule:\n",
    "$$\n",
    "\\frac{\\text{d}(\\mathbf{y} - X\\beta)^T(\\mathbf{y} - X\\beta)}{\\text{d} \\beta} = 0.\n",
    "$$\n",
    "Řešením je klasický výsledek:\n",
    "$$\n",
    "\\widehat{\\beta}_{OLS} = (X^TX)^{-1}X^T\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "### Úkoly\n",
    "- Napadne Vás, v čem je záludnost výrazu $\\widehat{\\beta}_{OLS}$?\n",
    "\n",
    "### Cvičný příklad\n",
    "Matice $X$ generována náhodně, $\\beta_{true}$ vytvořeno jako daný vektor. Vektor měření $\\mathbf{y}$ pak mohu získáme jako $\\mathbf{y} = X\\beta_{true} + \\text{šum}$, kde šum generujeme jako $č\\times\\mathcal{N}(0,1)$. Data pak vypadají následovně:\n",
    "\n",
    "<img src=\"img_ot/l4_y.png\"> | = | <img src=\"img_ot/l4_X_ok.png\"> | <img src=\"img_ot/l4_beta_true.png\">\n",
    "- | - | - | -\n",
    "$\\mathbf{y}$ | = |<p align=\"center\"> $X$ |<p align=\"center\"> $\\beta$\n",
    "\n",
    "Řešení nejmenšími čtverci $\\widehat{\\beta}_{OLS} = (X^TX)^{-1}X^T\\mathbf{y}$:\n",
    "\n",
    "<img src=\"img_ot/l4_ols_ideal.png\">\n",
    "\n",
    "Potud by bylo všechno krásné, ale co když máme špatně podmíněná data? Na následujícím příkladu vidíme, co se stane s vlastními čísly výrazu $X^TX$, když vydělíme první 3 sloupce matice $X$ číslem 100 (a příslušně přepočítáme $\\mathbf{y}$):\n",
    "\n",
    "<p align=\"center\">Původní data: | <p align=\"center\">Špatně podmíněná data:\n",
    "- | -\n",
    "<img src=\"img_ot/l4_X_ok.png\"> | <img src=\"img_ot/l4_X_illcond.png\">\n",
    "<img src=\"img_ot/l4_Xvlc_ok.png\"> | <img src=\"img_ot/l4_Xvlc_illcond.png\">\n",
    "\n",
    "Řešením nejmenšími čtverci na špatně podmíněných datech dostáváme\n",
    "\n",
    "<img src=\"img_ot/l4_ols_illcond.png\">\n",
    "\n",
    "Problém je v tom, že velká změna v odhadu $\\widehat{\\beta}_{OLS}$ na prvních 3 místech má minimální vliv na zpětnou rekonstrukci měření.\n",
    "\n",
    "<img src=\"img_ot/l4_ols_reconstruction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizace\n",
    "\n",
    "Regularizace obecně brání přefitování problému tím, že do problému přidáme dodatečnou informaci (např. ve formě penalizace, předpokladu o nějaké hodnotě, atd.). \n",
    "$$\n",
    "\\min_{podminka} ||\\mathbf{y} - X\\beta|| + g(\\beta)\n",
    "$$\n",
    "Samozřejmě si musíme být vědomi toho, že přidáním regularizace do úlohy zásadně měníme a ovlivňujeme řešení. Typicky si přejeme např.:\n",
    "- řídké řešení\n",
    "- hladké řešení\n",
    "- pozitivní řešení\n",
    "My se nejprve podíváme na klasické metody pohledem optimalizace a následně pohledem bayesovským."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizace\n",
    "\n",
    "Problém lineární regrese můžeme formulovat jako optimalizační problém následovně\n",
    "$$\n",
    "\\widehat{\\beta}=\\arg\\min_{\\beta}||X\\beta-\\mathbf{y}||_2^2,\n",
    "$$\n",
    "kde $||.||_2$ je Eukleidovská norma. Potom je to ekvivalentní s nejmenšími čtverci, neboť $||\\mathbf{a}||_2^2 = \\sqrt{\\sum a_j^2}^2 = \\mathbf{a}^T\\mathbf{a}$ a vede na identické řešení.\n",
    "\n",
    "<img src=\"img_ot/l4_optim.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizace s Tichonovovou regularizací\n",
    "\n",
    "Též lze najít pod názvem \"ridge regression\". Obecně můžeme zapsat optimalizační problém s Tichonovovským členem jako \n",
    "$$\n",
    "\\widehat{\\beta}_{Tichonov}=\\arg\\min_{\\beta}||\\mathbf{y}-X\\beta||_{2}^{2}+||\\Gamma\\beta||_{2}^{2},\n",
    "$$\n",
    "pro nějakou vhodnou matici $\\Gamma$. V reálu se často setkáme s nejjednodušší volbou a to $\\Gamma=\\alpha I$ ($I$ značí jednotkovou matici příslušného rozměru), což má smysl penalizace velikosti koeficientů $\\beta_i$. V tom případě se dokonce dostáváme k možnosti analytického řešení pomocí derivace podle $\\beta$\n",
    "$$\n",
    "\\widehat{\\beta}_{Tichonov}=\\left(X^{T}X+\\alpha^{2}I\\right)^{-1}X^{T}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "### Úkoly\n",
    "- co vyřešila tato regularizace za problém?\n",
    "- jaký nový problém nám naopak vnesla do úlohy?\n",
    "\n",
    "Řešení na špatně podmíněných datech:\n",
    "<img src=\"img_ot/l4_tichonov_all.png\">\n",
    "...pro $\\alpha \\in 10^{-3}..10^{+3}$\n",
    "\n",
    "<img src=\"img_ot/l4_tichonov1e-2.png\"> | <img src=\"img_ot/l4_tichonov1e-0.png\"> | <img src=\"img_ot/l4_tichonov1ep2.png\">\n",
    "- | - | -\n",
    "<p align=\"center\">$\\alpha = 10^{-2}$ | <p align=\"center\">$\\alpha = 10^0$ | <p align=\"center\">$\\alpha = 10^2$\n",
    "    \n",
    "K hledání ideální hodnoty $\\alpha$ můžeme použít heuristiku:\n",
    "- L křivka\n",
    "- cross-validace\n",
    "- kouknu a vidím :-)\n",
    "\n",
    "Použití L křivky vypadá následovně:\n",
    "\n",
    "<img src=\"img_ot/l4_tichonov_Lkrivka.png\"> <img src=\"img_ot/l4_tichonov.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizace s LASSO regularizací\n",
    "\n",
    "LASSO = least absolute shrinkage and selection operator. Jedná se o regularizaci absolutní hodnoty $\\beta_i$, což vysvětluje i slovo \"selection\" v názvu. Nulová absolutní hodnota značí zamítnutí $\\beta_i$, tzn. $\\beta_i = 0$, nenulová hodnota naopak přijetí $\\beta_i$. Preference nulových hodnot $\\beta$ vede na řídká řešení úlohy, což má velký aplikační význam.\n",
    "\n",
    "Předchůdce regularizace pomocí LASSO byla \"stepwise regression\", kde se v každém kroku algoritmu zvažovalo přijmutí/vyhození nějaké podmnožiny $\\beta_1,\\dots,\\beta_n$, např. na základě statistických testů nebo vhodných kritérií. \n",
    "\n",
    "Zápis optimalizačního problému s LASSO regularizací je následující\n",
    "$$\n",
    "\\widehat{\\beta}_{LASSO}=\\arg\\min_{\\beta}||\\mathbf{y}-X\\beta||_{2}^{2}+\\alpha||\\beta||_{1},\n",
    "$$\n",
    "kde $||\\mathbf{a}||_{1} = \\sum|a_j|$.\n",
    "\n",
    "My si opět ukážeme řešení pro rozsah parametru $\\alpha \\in 10^{-3}..10^{+3}$\n",
    "<img src=\"img_ot/l4_lasso_all.png\">\n",
    "\n",
    "K hledání hodnoty $\\alpha$ si opět vezmeme L křivku\n",
    "<img src=\"img_ot/l4_lasso_Lkrivka.png\">\n",
    "\n",
    "A dostáváme řešení:\n",
    "\n",
    "<p align=\"center\"> LASSO | <p align=\"center\"> Tichonov\n",
    "- | -\n",
    "<img src=\"img_ot/l4_lasso.png\"> | <img src=\"img_ot/l4_tichonov.png\">\n",
    "    \n",
    "### Úkoly\n",
    "- jakým způsobem můžeme \"favorizovat\" hladkost?\n",
    "\n",
    "<img src=\"img_ot/l4_smooth.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimalizace: další možnosti\n",
    "\n",
    "Nic nám nebrání konstruovat složitější optimalizační problémy, hlavní limitací je ovšem schopnost jejich řešení. Doposud jsme konstruovali problémy konvexní (graf leží nad/pod tečnou), kde optimalizovat je poměrně snadné. Existují řešiče, zde byl použit CVX toolbox do Matlabu. Nekonvexní optimalizace je mimo rozsah tohoto předmětu.\n",
    "\n",
    "Z dalších (konvexních) možností stojí za zmínku \"elastic net\" s problémem formulovaným jako\n",
    "$$\n",
    "\\widehat{\\beta}=\\arg\\min_{\\beta}||\\mathbf{y}-X\\beta||_{2}^{2}+\\alpha_{1}||\\beta||_{2}^{2}+\\alpha_{2}||\\beta||_{1},\n",
    "$$\n",
    "ovšem za cenu 2 ladících parametrů."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
